{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-atf3gekcgR"
   },
   "source": [
    "# Assessment 1: I can train and deploy a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7wkT17FkmU6"
   },
   "source": [
    "At this point, you've worked through a full deep learning workflow. You've loaded a dataset, trained a model, and deployed your model into a simple application. Validate your learning by attempting to replicate that workflow with a new problem.\n",
    "\n",
    "We've included a dataset which consists of two classes:  \n",
    "\n",
    "1) Face: Contains images which include the face of a whale  \n",
    "2) Not Face: Contains images which do not include the face of a whale.  \n",
    "\n",
    "The dataset is located at ```/dli/data/whale/data/train```.\n",
    "\n",
    "Your challenge is:\n",
    "\n",
    "1) Use [DIGITS](/digits) to train a model to identify *new* whale faces with an accuracy of more than 80%.   \n",
    "\n",
    "2) Deploy your model by modifying and saving the python application [submission.py](../../../../edit/tasks/task-assessment/task/submission.py) to return the word \"whale\" if the image contains a whale's face and \"not whale\" if the image does not.  \n",
    "\n",
    "Resources:\n",
    "\n",
    "1) [Train a model](../../task1/task/Train%20a%20Model.ipynb)  \n",
    "2) [New Data as a goal](../../task2/task/New%20Data%20as%20a%20Goal.ipynb)  \n",
    "3) [Deployment](../../task3/task/Deployment.ipynb)  \n",
    "\n",
    "Suggestions: \n",
    "\n",
    "- Use empty code blocks to find out any informantion necessary to solve this problem: eg: ```!ls [directorypath] prints the files in a given directory``` \n",
    "- Executing the first two cells below will run your python script with test images, the first should return \"whale\" and the second should return \"not whale\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaaY1Vb3o3mC"
   },
   "source": [
    "Start in [DIGITS](/digits/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caffe_output.log\t      snapshot_iter_540.caffemodel\r\n",
      "deploy.prototxt\t\t      snapshot_iter_594.caffemodel\r\n",
      "original.prototxt\t      snapshot_iter_648.caffemodel\r\n",
      "snapshot_iter_108.caffemodel  snapshot_iter_702.caffemodel\r\n",
      "snapshot_iter_162.caffemodel  snapshot_iter_756.caffemodel\r\n",
      "snapshot_iter_216.caffemodel  snapshot_iter_810.caffemodel\r\n",
      "snapshot_iter_270.caffemodel  snapshot_iter_864.caffemodel\r\n",
      "snapshot_iter_324.caffemodel  snapshot_iter_864.solverstate\r\n",
      "snapshot_iter_378.caffemodel  solver.prototxt\r\n",
      "snapshot_iter_432.caffemodel  status.pickle\r\n",
      "snapshot_iter_486.caffemodel  train_val.prototxt\r\n",
      "snapshot_iter_54.caffemodel\r\n"
     ]
    }
   ],
   "source": [
    "# Load modules & set objects for dataset and model\n",
    "import caffe\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "MODEL_JOB_DIR = '/dli/data/digits/20191211-133538-7c1a'  ## Set this to be the job number for your model\n",
    "DATASET_JOB_DIR = '/dli/data/digits/20191211-133147-8dc0'  ## Set this to be the job number for your dataset\n",
    "\n",
    "!ls $MODEL_JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_train_db.log  labels.txt        mean.jpg       train.txt  val.txt\r\n",
      "create_val_db.log    mean.binaryproto  status.pickle  train_db\t val_db\r\n"
     ]
    }
   ],
   "source": [
    "!ls $DATASET_JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = MODEL_JOB_DIR + '/deploy.prototxt'\n",
    "WEIGHTS = MODEL_JOB_DIR + '/snapshot_iter_270.caffemodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not whale\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "def deploy(img_path):\n",
    "\n",
    "    caffe.set_mode_gpu()\n",
    "    \n",
    "    # Initialize the Caffe model using the model trained in DIGITS. Which two files constitute your trained model?\n",
    "    net = caffe.Classifier(ARCHITECTURE, WEIGHTS,\n",
    "                           channel_swap=(2,1,0),\n",
    "                           raw_scale=255,\n",
    "                           image_dims=(256, 256)) \n",
    "    \n",
    "    # Create an input that the network expects. \n",
    "    input_image = caffe.io.load_image(DATASET_JOB_DIR + '/mean.jpg')\n",
    "    input_image = cv2.resize(input_image, (256,256))\n",
    "    mean_image = caffe.io.load_image('/dli/data/digits/20191211-133147-8dc0/mean.jpg')\n",
    "    ready_image = input_image-mean_image\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = net.predict([ready_image])\n",
    "\n",
    "    # Create an output that is useful to a user. What is the condition that should return \"whale\" vs. \"not whale\"?\n",
    "    if prediction.argmax() == 0:\n",
    "        return \"whale\"\n",
    "    else:\n",
    "        return \"not whale\"\n",
    "    \n",
    "# Ignore this part    \n",
    "if __name__ == \"__main__\":\n",
    "    print(deploy(sys.argv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1211 14:09:21.164949   291 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I1211 14:09:21.165693   291 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11539775488, dev_info[0]: total=11996954624 free=11539775488\n",
      "W1211 14:09:21.165751   291 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W1211 14:09:21.165880   291 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W1211 14:09:21.165896   291 _caffe.cpp:175] Net('/dli/data/digits/20191211-133538-7c1a/deploy.prototxt', 1, weights='/dli/data/digits/20191211-133538-7c1a/snapshot_iter_270.caffemodel')\n",
      "I1211 14:09:21.166221   291 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20191211-133538-7c1a/deploy.prototxt\n",
      "I1211 14:09:21.166255   291 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W1211 14:09:21.166266   291 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I1211 14:09:21.175742   291 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I1211 14:09:21.176256   291 net.cpp:109] Using FLOAT as default forward math type\n",
      "I1211 14:09:21.176275   291 net.cpp:115] Using FLOAT as default backward math type\n",
      "I1211 14:09:21.176288   291 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I1211 14:09:21.176306   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.176326   291 net.cpp:199] Created Layer input (0)\n",
      "I1211 14:09:21.176345   291 net.cpp:541] input -> data\n",
      "I1211 14:09:21.177047   291 net.cpp:259] Setting up input\n",
      "I1211 14:09:21.177083   291 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I1211 14:09:21.177107   291 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I1211 14:09:21.177119   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.177155   291 net.cpp:199] Created Layer conv1 (1)\n",
      "I1211 14:09:21.177171   291 net.cpp:571] conv1 <- data\n",
      "I1211 14:09:21.177184   291 net.cpp:541] conv1 -> conv1\n",
      "I1211 14:09:21.707235   291 net.cpp:259] Setting up conv1\n",
      "I1211 14:09:21.707284   291 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I1211 14:09:21.707316   291 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I1211 14:09:21.707334   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.707348   291 net.cpp:199] Created Layer relu1 (2)\n",
      "I1211 14:09:21.707360   291 net.cpp:571] relu1 <- conv1\n",
      "I1211 14:09:21.707368   291 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I1211 14:09:21.707393   291 net.cpp:259] Setting up relu1\n",
      "I1211 14:09:21.707406   291 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I1211 14:09:21.707412   291 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I1211 14:09:21.707423   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.707448   291 net.cpp:199] Created Layer norm1 (3)\n",
      "I1211 14:09:21.707458   291 net.cpp:571] norm1 <- conv1\n",
      "I1211 14:09:21.707464   291 net.cpp:541] norm1 -> norm1\n",
      "I1211 14:09:21.707521   291 net.cpp:259] Setting up norm1\n",
      "I1211 14:09:21.707537   291 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I1211 14:09:21.707551   291 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I1211 14:09:21.707563   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.707610   291 net.cpp:199] Created Layer pool1 (4)\n",
      "I1211 14:09:21.707619   291 net.cpp:571] pool1 <- norm1\n",
      "I1211 14:09:21.707628   291 net.cpp:541] pool1 -> pool1\n",
      "I1211 14:09:21.707689   291 net.cpp:259] Setting up pool1\n",
      "I1211 14:09:21.707703   291 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I1211 14:09:21.707717   291 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I1211 14:09:21.707728   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.707751   291 net.cpp:199] Created Layer conv2 (5)\n",
      "I1211 14:09:21.707762   291 net.cpp:571] conv2 <- pool1\n",
      "I1211 14:09:21.707774   291 net.cpp:541] conv2 -> conv2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1211 14:09:21.714843   291 net.cpp:259] Setting up conv2\n",
      "I1211 14:09:21.714872   291 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I1211 14:09:21.714893   291 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I1211 14:09:21.714907   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.714916   291 net.cpp:199] Created Layer relu2 (6)\n",
      "I1211 14:09:21.714927   291 net.cpp:571] relu2 <- conv2\n",
      "I1211 14:09:21.714946   291 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I1211 14:09:21.714962   291 net.cpp:259] Setting up relu2\n",
      "I1211 14:09:21.714973   291 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I1211 14:09:21.714984   291 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I1211 14:09:21.714995   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.715013   291 net.cpp:199] Created Layer norm2 (7)\n",
      "I1211 14:09:21.715023   291 net.cpp:571] norm2 <- conv2\n",
      "I1211 14:09:21.715039   291 net.cpp:541] norm2 -> norm2\n",
      "I1211 14:09:21.715095   291 net.cpp:259] Setting up norm2\n",
      "I1211 14:09:21.715111   291 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I1211 14:09:21.715121   291 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I1211 14:09:21.715126   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.715137   291 net.cpp:199] Created Layer pool2 (8)\n",
      "I1211 14:09:21.715148   291 net.cpp:571] pool2 <- norm2\n",
      "I1211 14:09:21.715154   291 net.cpp:541] pool2 -> pool2\n",
      "I1211 14:09:21.715220   291 net.cpp:259] Setting up pool2\n",
      "I1211 14:09:21.715235   291 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I1211 14:09:21.715248   291 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I1211 14:09:21.715260   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.715281   291 net.cpp:199] Created Layer conv3 (9)\n",
      "I1211 14:09:21.715291   291 net.cpp:571] conv3 <- pool2\n",
      "I1211 14:09:21.715302   291 net.cpp:541] conv3 -> conv3\n",
      "I1211 14:09:21.731072   291 net.cpp:259] Setting up conv3\n",
      "I1211 14:09:21.731106   291 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I1211 14:09:21.731125   291 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I1211 14:09:21.731138   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.731154   291 net.cpp:199] Created Layer relu3 (10)\n",
      "I1211 14:09:21.731165   291 net.cpp:571] relu3 <- conv3\n",
      "I1211 14:09:21.731173   291 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I1211 14:09:21.731189   291 net.cpp:259] Setting up relu3\n",
      "I1211 14:09:21.731201   291 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I1211 14:09:21.731212   291 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I1211 14:09:21.731225   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.731251   291 net.cpp:199] Created Layer conv4 (11)\n",
      "I1211 14:09:21.731261   291 net.cpp:571] conv4 <- conv3\n",
      "I1211 14:09:21.731273   291 net.cpp:541] conv4 -> conv4\n",
      "I1211 14:09:21.743675   291 net.cpp:259] Setting up conv4\n",
      "I1211 14:09:21.743705   291 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I1211 14:09:21.743752   291 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I1211 14:09:21.743764   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.743782   291 net.cpp:199] Created Layer relu4 (12)\n",
      "I1211 14:09:21.743794   291 net.cpp:571] relu4 <- conv4\n",
      "I1211 14:09:21.743806   291 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I1211 14:09:21.743822   291 net.cpp:259] Setting up relu4\n",
      "I1211 14:09:21.743834   291 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I1211 14:09:21.743849   291 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I1211 14:09:21.743860   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.743885   291 net.cpp:199] Created Layer conv5 (13)\n",
      "I1211 14:09:21.743896   291 net.cpp:571] conv5 <- conv4\n",
      "I1211 14:09:21.743908   291 net.cpp:541] conv5 -> conv5\n",
      "I1211 14:09:21.752002   291 net.cpp:259] Setting up conv5\n",
      "I1211 14:09:21.752032   291 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I1211 14:09:21.752053   291 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I1211 14:09:21.752066   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.752086   291 net.cpp:199] Created Layer relu5 (14)\n",
      "I1211 14:09:21.752097   291 net.cpp:571] relu5 <- conv5\n",
      "I1211 14:09:21.752110   291 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I1211 14:09:21.752125   291 net.cpp:259] Setting up relu5\n",
      "I1211 14:09:21.752135   291 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I1211 14:09:21.752147   291 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I1211 14:09:21.752157   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.752177   291 net.cpp:199] Created Layer pool5 (15)\n",
      "I1211 14:09:21.752187   291 net.cpp:571] pool5 <- conv5\n",
      "I1211 14:09:21.752198   291 net.cpp:541] pool5 -> pool5\n",
      "I1211 14:09:21.752272   291 net.cpp:259] Setting up pool5\n",
      "I1211 14:09:21.752290   291 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I1211 14:09:21.752300   291 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I1211 14:09:21.752307   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:21.752326   291 net.cpp:199] Created Layer fc6 (16)\n",
      "I1211 14:09:21.752338   291 net.cpp:571] fc6 <- pool5\n",
      "I1211 14:09:21.752344   291 net.cpp:541] fc6 -> fc6\n",
      "I1211 14:09:22.429543   291 net.cpp:259] Setting up fc6\n",
      "I1211 14:09:22.429589   291 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I1211 14:09:22.429613   291 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I1211 14:09:22.429627   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:22.429646   291 net.cpp:199] Created Layer relu6 (17)\n",
      "I1211 14:09:22.429657   291 net.cpp:571] relu6 <- fc6\n",
      "I1211 14:09:22.429672   291 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I1211 14:09:22.429690   291 net.cpp:259] Setting up relu6\n",
      "I1211 14:09:22.429702   291 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I1211 14:09:22.429713   291 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I1211 14:09:22.429725   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:22.429744   291 net.cpp:199] Created Layer drop6 (18)\n",
      "I1211 14:09:22.429755   291 net.cpp:571] drop6 <- fc6\n",
      "I1211 14:09:22.429766   291 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I1211 14:09:22.464303   291 net.cpp:259] Setting up drop6\n",
      "I1211 14:09:22.464351   291 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I1211 14:09:22.464365   291 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I1211 14:09:22.464385   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:22.464401   291 net.cpp:199] Created Layer fc7 (19)\n",
      "I1211 14:09:22.464442   291 net.cpp:571] fc7 <- fc6\n",
      "I1211 14:09:22.464452   291 net.cpp:541] fc7 -> fc7\n",
      "I1211 14:09:22.767021   291 net.cpp:259] Setting up fc7\n",
      "I1211 14:09:22.767072   291 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I1211 14:09:22.767096   291 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I1211 14:09:22.767112   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:22.767124   291 net.cpp:199] Created Layer relu7 (20)\n",
      "I1211 14:09:22.767138   291 net.cpp:571] relu7 <- fc7\n",
      "I1211 14:09:22.767146   291 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I1211 14:09:22.767166   291 net.cpp:259] Setting up relu7\n",
      "I1211 14:09:22.767177   291 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I1211 14:09:22.767192   291 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I1211 14:09:22.767200   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:22.767216   291 net.cpp:199] Created Layer drop7 (21)\n",
      "I1211 14:09:22.767226   291 net.cpp:571] drop7 <- fc7\n",
      "I1211 14:09:22.767233   291 net.cpp:526] drop7 -> fc7 (in-place)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1211 14:09:22.801879   291 net.cpp:259] Setting up drop7\n",
      "I1211 14:09:22.801920   291 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I1211 14:09:22.801935   291 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I1211 14:09:22.801951   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:22.801970   291 net.cpp:199] Created Layer fc8 (22)\n",
      "I1211 14:09:22.801982   291 net.cpp:571] fc8 <- fc7\n",
      "I1211 14:09:22.801992   291 net.cpp:541] fc8 -> fc8\n",
      "I1211 14:09:22.803000   291 net.cpp:259] Setting up fc8\n",
      "I1211 14:09:22.803025   291 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I1211 14:09:22.803043   291 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I1211 14:09:22.803056   291 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:22.803076   291 net.cpp:199] Created Layer softmax (23)\n",
      "I1211 14:09:22.803087   291 net.cpp:571] softmax <- fc8\n",
      "I1211 14:09:22.803094   291 net.cpp:541] softmax -> softmax\n",
      "I1211 14:09:22.803179   291 net.cpp:259] Setting up softmax\n",
      "I1211 14:09:22.803195   291 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I1211 14:09:22.803203   291 net.cpp:337] softmax does not need backward computation.\n",
      "I1211 14:09:22.803215   291 net.cpp:337] fc8 does not need backward computation.\n",
      "I1211 14:09:22.803220   291 net.cpp:337] drop7 does not need backward computation.\n",
      "I1211 14:09:22.803231   291 net.cpp:337] relu7 does not need backward computation.\n",
      "I1211 14:09:22.803236   291 net.cpp:337] fc7 does not need backward computation.\n",
      "I1211 14:09:22.803246   291 net.cpp:337] drop6 does not need backward computation.\n",
      "I1211 14:09:22.803252   291 net.cpp:337] relu6 does not need backward computation.\n",
      "I1211 14:09:22.803262   291 net.cpp:337] fc6 does not need backward computation.\n",
      "I1211 14:09:22.803268   291 net.cpp:337] pool5 does not need backward computation.\n",
      "I1211 14:09:22.803277   291 net.cpp:337] relu5 does not need backward computation.\n",
      "I1211 14:09:22.803282   291 net.cpp:337] conv5 does not need backward computation.\n",
      "I1211 14:09:22.803293   291 net.cpp:337] relu4 does not need backward computation.\n",
      "I1211 14:09:22.803299   291 net.cpp:337] conv4 does not need backward computation.\n",
      "I1211 14:09:22.803304   291 net.cpp:337] relu3 does not need backward computation.\n",
      "I1211 14:09:22.803314   291 net.cpp:337] conv3 does not need backward computation.\n",
      "I1211 14:09:22.803321   291 net.cpp:337] pool2 does not need backward computation.\n",
      "I1211 14:09:22.803329   291 net.cpp:337] norm2 does not need backward computation.\n",
      "I1211 14:09:22.803340   291 net.cpp:337] relu2 does not need backward computation.\n",
      "I1211 14:09:22.803345   291 net.cpp:337] conv2 does not need backward computation.\n",
      "I1211 14:09:22.803352   291 net.cpp:337] pool1 does not need backward computation.\n",
      "I1211 14:09:22.803390   291 net.cpp:337] norm1 does not need backward computation.\n",
      "I1211 14:09:22.803397   291 net.cpp:337] relu1 does not need backward computation.\n",
      "I1211 14:09:22.803405   291 net.cpp:337] conv1 does not need backward computation.\n",
      "I1211 14:09:22.803416   291 net.cpp:337] input does not need backward computation.\n",
      "I1211 14:09:22.803421   291 net.cpp:379] This network produces output softmax\n",
      "I1211 14:09:22.803452   291 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I1211 14:09:22.803464   291 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I1211 14:09:22.803470   291 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I1211 14:09:22.803480   291 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I1211 14:09:22.803485   291 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I1211 14:09:22.803495   291 net.cpp:420] Network initialization done.\n",
      "I1211 14:09:22.913014   291 net.cpp:1129] Ignoring source layer train-data\n",
      "I1211 14:09:22.913058   291 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I1211 14:09:22.913149   291 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I1211 14:09:22.913164   291 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I1211 14:09:22.913172   291 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I1211 14:09:22.913182   291 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I1211 14:09:22.913358   291 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I1211 14:09:22.913372   291 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I1211 14:09:22.913379   291 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I1211 14:09:22.913386   291 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I1211 14:09:22.913833   291 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I1211 14:09:22.913847   291 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I1211 14:09:22.914194   291 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I1211 14:09:22.914209   291 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I1211 14:09:22.914450   291 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I1211 14:09:22.914464   291 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I1211 14:09:22.914471   291 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n",
      "I1211 14:09:22.933053   291 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I1211 14:09:22.933089   291 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I1211 14:09:22.933104   291 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I1211 14:09:22.941274   291 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I1211 14:09:22.941308   291 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I1211 14:09:22.941320   291 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I1211 14:09:22.941355   291 net.cpp:1129] Ignoring source layer loss\n",
      "whale\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/face/w_1.jpg'  # This should return \"whale\" at the very bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1211 14:09:31.992161   306 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I1211 14:09:31.992938   306 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11539775488, dev_info[0]: total=11996954624 free=11539775488\n",
      "W1211 14:09:31.993012   306 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W1211 14:09:31.993147   306 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W1211 14:09:31.993166   306 _caffe.cpp:175] Net('/dli/data/digits/20191211-133538-7c1a/deploy.prototxt', 1, weights='/dli/data/digits/20191211-133538-7c1a/snapshot_iter_270.caffemodel')\n",
      "I1211 14:09:31.993499   306 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20191211-133538-7c1a/deploy.prototxt\n",
      "I1211 14:09:31.993535   306 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W1211 14:09:31.993549   306 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I1211 14:09:32.003969   306 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I1211 14:09:32.004360   306 net.cpp:109] Using FLOAT as default forward math type\n",
      "I1211 14:09:32.004377   306 net.cpp:115] Using FLOAT as default backward math type\n",
      "I1211 14:09:32.004386   306 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I1211 14:09:32.004400   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.004415   306 net.cpp:199] Created Layer input (0)\n",
      "I1211 14:09:32.004426   306 net.cpp:541] input -> data\n",
      "I1211 14:09:32.005134   306 net.cpp:259] Setting up input\n",
      "I1211 14:09:32.005162   306 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I1211 14:09:32.005180   306 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I1211 14:09:32.005198   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.005235   306 net.cpp:199] Created Layer conv1 (1)\n",
      "I1211 14:09:32.005249   306 net.cpp:571] conv1 <- data\n",
      "I1211 14:09:32.005260   306 net.cpp:541] conv1 -> conv1\n",
      "I1211 14:09:32.540510   306 net.cpp:259] Setting up conv1\n",
      "I1211 14:09:32.540566   306 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I1211 14:09:32.540607   306 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I1211 14:09:32.540625   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.540640   306 net.cpp:199] Created Layer relu1 (2)\n",
      "I1211 14:09:32.540652   306 net.cpp:571] relu1 <- conv1\n",
      "I1211 14:09:32.540660   306 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I1211 14:09:32.540688   306 net.cpp:259] Setting up relu1\n",
      "I1211 14:09:32.540701   306 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I1211 14:09:32.540710   306 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I1211 14:09:32.540721   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.540757   306 net.cpp:199] Created Layer norm1 (3)\n",
      "I1211 14:09:32.540771   306 net.cpp:571] norm1 <- conv1\n",
      "I1211 14:09:32.540781   306 net.cpp:541] norm1 -> norm1\n",
      "I1211 14:09:32.540855   306 net.cpp:259] Setting up norm1\n",
      "I1211 14:09:32.540896   306 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I1211 14:09:32.540913   306 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I1211 14:09:32.540925   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.540980   306 net.cpp:199] Created Layer pool1 (4)\n",
      "I1211 14:09:32.540998   306 net.cpp:571] pool1 <- norm1\n",
      "I1211 14:09:32.541010   306 net.cpp:541] pool1 -> pool1\n",
      "I1211 14:09:32.541085   306 net.cpp:259] Setting up pool1\n",
      "I1211 14:09:32.541105   306 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I1211 14:09:32.541118   306 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I1211 14:09:32.541134   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.541159   306 net.cpp:199] Created Layer conv2 (5)\n",
      "I1211 14:09:32.541172   306 net.cpp:571] conv2 <- pool1\n",
      "I1211 14:09:32.541185   306 net.cpp:541] conv2 -> conv2\n",
      "I1211 14:09:32.548317   306 net.cpp:259] Setting up conv2\n",
      "I1211 14:09:32.548358   306 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I1211 14:09:32.548385   306 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I1211 14:09:32.548405   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.548421   306 net.cpp:199] Created Layer relu2 (6)\n",
      "I1211 14:09:32.548437   306 net.cpp:571] relu2 <- conv2\n",
      "I1211 14:09:32.548451   306 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I1211 14:09:32.548475   306 net.cpp:259] Setting up relu2\n",
      "I1211 14:09:32.548491   306 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I1211 14:09:32.548503   306 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I1211 14:09:32.548519   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.548540   306 net.cpp:199] Created Layer norm2 (7)\n",
      "I1211 14:09:32.548553   306 net.cpp:571] norm2 <- conv2\n",
      "I1211 14:09:32.548570   306 net.cpp:541] norm2 -> norm2\n",
      "I1211 14:09:32.548637   306 net.cpp:259] Setting up norm2\n",
      "I1211 14:09:32.548657   306 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I1211 14:09:32.548671   306 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I1211 14:09:32.548686   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.548705   306 net.cpp:199] Created Layer pool2 (8)\n",
      "I1211 14:09:32.548720   306 net.cpp:571] pool2 <- norm2\n",
      "I1211 14:09:32.548730   306 net.cpp:541] pool2 -> pool2\n",
      "I1211 14:09:32.548802   306 net.cpp:259] Setting up pool2\n",
      "I1211 14:09:32.548821   306 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I1211 14:09:32.548840   306 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I1211 14:09:32.548856   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.548890   306 net.cpp:199] Created Layer conv3 (9)\n",
      "I1211 14:09:32.548903   306 net.cpp:571] conv3 <- pool2\n",
      "I1211 14:09:32.548911   306 net.cpp:541] conv3 -> conv3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1211 14:09:32.565968   306 net.cpp:259] Setting up conv3\n",
      "I1211 14:09:32.566006   306 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I1211 14:09:32.566040   306 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I1211 14:09:32.566061   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.566078   306 net.cpp:199] Created Layer relu3 (10)\n",
      "I1211 14:09:32.566094   306 net.cpp:571] relu3 <- conv3\n",
      "I1211 14:09:32.566107   306 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I1211 14:09:32.566130   306 net.cpp:259] Setting up relu3\n",
      "I1211 14:09:32.566148   306 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I1211 14:09:32.566159   306 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I1211 14:09:32.566174   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.566202   306 net.cpp:199] Created Layer conv4 (11)\n",
      "I1211 14:09:32.566215   306 net.cpp:571] conv4 <- conv3\n",
      "I1211 14:09:32.566226   306 net.cpp:541] conv4 -> conv4\n",
      "I1211 14:09:32.578807   306 net.cpp:259] Setting up conv4\n",
      "I1211 14:09:32.578853   306 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I1211 14:09:32.578918   306 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I1211 14:09:32.578938   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.578960   306 net.cpp:199] Created Layer relu4 (12)\n",
      "I1211 14:09:32.578979   306 net.cpp:571] relu4 <- conv4\n",
      "I1211 14:09:32.578989   306 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I1211 14:09:32.579006   306 net.cpp:259] Setting up relu4\n",
      "I1211 14:09:32.579020   306 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I1211 14:09:32.579028   306 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I1211 14:09:32.579041   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.579067   306 net.cpp:199] Created Layer conv5 (13)\n",
      "I1211 14:09:32.579079   306 net.cpp:571] conv5 <- conv4\n",
      "I1211 14:09:32.579087   306 net.cpp:541] conv5 -> conv5\n",
      "I1211 14:09:32.587271   306 net.cpp:259] Setting up conv5\n",
      "I1211 14:09:32.587301   306 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I1211 14:09:32.587332   306 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I1211 14:09:32.587354   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.587369   306 net.cpp:199] Created Layer relu5 (14)\n",
      "I1211 14:09:32.587385   306 net.cpp:571] relu5 <- conv5\n",
      "I1211 14:09:32.587397   306 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I1211 14:09:32.587419   306 net.cpp:259] Setting up relu5\n",
      "I1211 14:09:32.587431   306 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I1211 14:09:32.587442   306 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I1211 14:09:32.587452   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.587473   306 net.cpp:199] Created Layer pool5 (15)\n",
      "I1211 14:09:32.587489   306 net.cpp:571] pool5 <- conv5\n",
      "I1211 14:09:32.587499   306 net.cpp:541] pool5 -> pool5\n",
      "I1211 14:09:32.587587   306 net.cpp:259] Setting up pool5\n",
      "I1211 14:09:32.587610   306 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I1211 14:09:32.587628   306 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I1211 14:09:32.587640   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:32.587667   306 net.cpp:199] Created Layer fc6 (16)\n",
      "I1211 14:09:32.587682   306 net.cpp:571] fc6 <- pool5\n",
      "I1211 14:09:32.587692   306 net.cpp:541] fc6 -> fc6\n",
      "I1211 14:09:33.264508   306 net.cpp:259] Setting up fc6\n",
      "I1211 14:09:33.264559   306 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I1211 14:09:33.264592   306 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I1211 14:09:33.264607   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:33.264626   306 net.cpp:199] Created Layer relu6 (17)\n",
      "I1211 14:09:33.264639   306 net.cpp:571] relu6 <- fc6\n",
      "I1211 14:09:33.264654   306 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I1211 14:09:33.264673   306 net.cpp:259] Setting up relu6\n",
      "I1211 14:09:33.264685   306 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I1211 14:09:33.264696   306 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I1211 14:09:33.264708   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:33.264727   306 net.cpp:199] Created Layer drop6 (18)\n",
      "I1211 14:09:33.264739   306 net.cpp:571] drop6 <- fc6\n",
      "I1211 14:09:33.264750   306 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I1211 14:09:33.299238   306 net.cpp:259] Setting up drop6\n",
      "I1211 14:09:33.299273   306 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I1211 14:09:33.299289   306 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I1211 14:09:33.299307   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:33.299338   306 net.cpp:199] Created Layer fc7 (19)\n",
      "I1211 14:09:33.299378   306 net.cpp:571] fc7 <- fc6\n",
      "I1211 14:09:33.299391   306 net.cpp:541] fc7 -> fc7\n",
      "I1211 14:09:33.600247   306 net.cpp:259] Setting up fc7\n",
      "I1211 14:09:33.600301   306 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I1211 14:09:33.600330   306 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I1211 14:09:33.600350   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:33.600368   306 net.cpp:199] Created Layer relu7 (20)\n",
      "I1211 14:09:33.600384   306 net.cpp:571] relu7 <- fc7\n",
      "I1211 14:09:33.600399   306 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I1211 14:09:33.600423   306 net.cpp:259] Setting up relu7\n",
      "I1211 14:09:33.600440   306 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I1211 14:09:33.600453   306 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I1211 14:09:33.600471   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:33.600489   306 net.cpp:199] Created Layer drop7 (21)\n",
      "I1211 14:09:33.600502   306 net.cpp:571] drop7 <- fc7\n",
      "I1211 14:09:33.600508   306 net.cpp:526] drop7 -> fc7 (in-place)\n",
      "I1211 14:09:33.635134   306 net.cpp:259] Setting up drop7\n",
      "I1211 14:09:33.635180   306 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I1211 14:09:33.635197   306 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I1211 14:09:33.635211   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:33.635232   306 net.cpp:199] Created Layer fc8 (22)\n",
      "I1211 14:09:33.635244   306 net.cpp:571] fc8 <- fc7\n",
      "I1211 14:09:33.635269   306 net.cpp:541] fc8 -> fc8\n",
      "I1211 14:09:33.636282   306 net.cpp:259] Setting up fc8\n",
      "I1211 14:09:33.636312   306 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I1211 14:09:33.636332   306 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I1211 14:09:33.636351   306 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I1211 14:09:33.636375   306 net.cpp:199] Created Layer softmax (23)\n",
      "I1211 14:09:33.636389   306 net.cpp:571] softmax <- fc8\n",
      "I1211 14:09:33.636400   306 net.cpp:541] softmax -> softmax\n",
      "I1211 14:09:33.636502   306 net.cpp:259] Setting up softmax\n",
      "I1211 14:09:33.636521   306 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I1211 14:09:33.636539   306 net.cpp:337] softmax does not need backward computation.\n",
      "I1211 14:09:33.636551   306 net.cpp:337] fc8 does not need backward computation.\n",
      "I1211 14:09:33.636561   306 net.cpp:337] drop7 does not need backward computation.\n",
      "I1211 14:09:33.636574   306 net.cpp:337] relu7 does not need backward computation.\n",
      "I1211 14:09:33.636581   306 net.cpp:337] fc7 does not need backward computation.\n",
      "I1211 14:09:33.636588   306 net.cpp:337] drop6 does not need backward computation.\n",
      "I1211 14:09:33.636598   306 net.cpp:337] relu6 does not need backward computation.\n",
      "I1211 14:09:33.636605   306 net.cpp:337] fc6 does not need backward computation.\n",
      "I1211 14:09:33.636612   306 net.cpp:337] pool5 does not need backward computation.\n",
      "I1211 14:09:33.636618   306 net.cpp:337] relu5 does not need backward computation.\n",
      "I1211 14:09:33.636628   306 net.cpp:337] conv5 does not need backward computation.\n",
      "I1211 14:09:33.636636   306 net.cpp:337] relu4 does not need backward computation.\n",
      "I1211 14:09:33.636646   306 net.cpp:337] conv4 does not need backward computation.\n",
      "I1211 14:09:33.636662   306 net.cpp:337] relu3 does not need backward computation.\n",
      "I1211 14:09:33.636677   306 net.cpp:337] conv3 does not need backward computation.\n",
      "I1211 14:09:33.636693   306 net.cpp:337] pool2 does not need backward computation.\n",
      "I1211 14:09:33.636706   306 net.cpp:337] norm2 does not need backward computation.\n",
      "I1211 14:09:33.636713   306 net.cpp:337] relu2 does not need backward computation.\n",
      "I1211 14:09:33.636723   306 net.cpp:337] conv2 does not need backward computation.\n",
      "I1211 14:09:33.636729   306 net.cpp:337] pool1 does not need backward computation.\n",
      "I1211 14:09:33.636766   306 net.cpp:337] norm1 does not need backward computation.\n",
      "I1211 14:09:33.636775   306 net.cpp:337] relu1 does not need backward computation.\n",
      "I1211 14:09:33.636781   306 net.cpp:337] conv1 does not need backward computation.\n",
      "I1211 14:09:33.636792   306 net.cpp:337] input does not need backward computation.\n",
      "I1211 14:09:33.636802   306 net.cpp:379] This network produces output softmax\n",
      "I1211 14:09:33.636834   306 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I1211 14:09:33.636845   306 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I1211 14:09:33.636864   306 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I1211 14:09:33.636874   306 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I1211 14:09:33.636911   306 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I1211 14:09:33.636920   306 net.cpp:420] Network initialization done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1211 14:09:33.747047   306 net.cpp:1129] Ignoring source layer train-data\n",
      "I1211 14:09:33.747090   306 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I1211 14:09:33.747189   306 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I1211 14:09:33.747205   306 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I1211 14:09:33.747215   306 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I1211 14:09:33.747228   306 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I1211 14:09:33.747411   306 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I1211 14:09:33.747427   306 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I1211 14:09:33.747437   306 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I1211 14:09:33.747450   306 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I1211 14:09:33.747916   306 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I1211 14:09:33.747933   306 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I1211 14:09:33.748287   306 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I1211 14:09:33.748303   306 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I1211 14:09:33.748545   306 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I1211 14:09:33.748562   306 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I1211 14:09:33.748571   306 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n",
      "I1211 14:09:33.766923   306 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I1211 14:09:33.766960   306 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I1211 14:09:33.766968   306 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I1211 14:09:33.775012   306 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I1211 14:09:33.775046   306 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I1211 14:09:33.775053   306 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I1211 14:09:33.775089   306 net.cpp:1129] Ignoring source layer loss\n",
      "not whale\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/not_face/w_1.jpg'  # This should return \"not whale\" at the very bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Assessment1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

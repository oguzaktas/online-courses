'''
Instructions
- Set the first dense layer to have 32 nodes, use a sigmoid activation function, and have an input shape of (784,).
- Use the root mean square propagation optimizer, a categorical crossentropy loss, and the accuracy metric.
- Set the number of epochs to 10 and use 10% of the dataset for validation.
'''

# Define sequential model
model = keras.Sequential()

# Define the first layer
model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))

# Add activation function to classifier
model.add(keras.layers.Dense(4, activation='softmax'))

# Set the optimizer, loss function, and metrics
model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Add the number of epochs and the validation split
model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)

'''
Output:
Train on 899 samples, validate on 100 samples
Epoch 1/10

 32/899 [>.............................] - ETA: 20s - loss: 1.3499 - accuracy: 0.2812
640/899 [====================>.........] - ETA: 0s - loss: 1.2891 - accuracy: 0.4375 
899/899 [==============================] - 1s 1ms/sample - loss: 1.2376 - accuracy: 0.4828 - val_loss: 1.1477 - val_accuracy: 0.6300
Epoch 2/10

 32/899 [>.............................] - ETA: 0s - loss: 1.0755 - accuracy: 0.6875
608/899 [===================>..........] - ETA: 0s - loss: 1.0058 - accuracy: 0.6711
899/899 [==============================] - 0s 108us/sample - loss: 0.9660 - accuracy: 0.6997 - val_loss: 1.0969 - val_accuracy: 0.3900
Epoch 3/10

 32/899 [>.............................] - ETA: 0s - loss: 1.2457 - accuracy: 0.2188
544/899 [=================>............] - ETA: 0s - loss: 0.8529 - accuracy: 0.7261
899/899 [==============================] - 0s 116us/sample - loss: 0.8260 - accuracy: 0.7486 - val_loss: 0.8497 - val_accuracy: 0.7100
Epoch 4/10

 32/899 [>.............................] - ETA: 0s - loss: 0.8660 - accuracy: 0.7188
704/899 [======================>.......] - ETA: 0s - loss: 0.6948 - accuracy: 0.8395
899/899 [==============================] - 0s 100us/sample - loss: 0.6855 - accuracy: 0.8398 - val_loss: 0.7114 - val_accuracy: 0.8600
Epoch 5/10

 32/899 [>.............................] - ETA: 0s - loss: 0.6828 - accuracy: 0.8750
704/899 [======================>.......] - ETA: 0s - loss: 0.5981 - accuracy: 0.8807
899/899 [==============================] - 0s 95us/sample - loss: 0.5913 - accuracy: 0.8810 - val_loss: 0.8717 - val_accuracy: 0.5800
Epoch 6/10

 32/899 [>.............................] - ETA: 0s - loss: 0.7129 - accuracy: 0.6250
672/899 [=====================>........] - ETA: 0s - loss: 0.5356 - accuracy: 0.8929
899/899 [==============================] - 0s 99us/sample - loss: 0.5184 - accuracy: 0.9010 - val_loss: 0.5335 - val_accuracy: 0.7500
Epoch 7/10

 32/899 [>.............................] - ETA: 0s - loss: 0.6109 - accuracy: 0.7812
576/899 [==================>...........] - ETA: 0s - loss: 0.4797 - accuracy: 0.9028
899/899 [==============================] - 0s 108us/sample - loss: 0.4617 - accuracy: 0.9088 - val_loss: 0.6604 - val_accuracy: 0.6700
Epoch 8/10

 32/899 [>.............................] - ETA: 0s - loss: 0.5398 - accuracy: 0.7812
640/899 [====================>.........] - ETA: 0s - loss: 0.4121 - accuracy: 0.9328
899/899 [==============================] - 0s 106us/sample - loss: 0.4094 - accuracy: 0.9399 - val_loss: 0.3965 - val_accuracy: 0.9600
Epoch 9/10

 32/899 [>.............................] - ETA: 0s - loss: 0.4075 - accuracy: 0.9062
672/899 [=====================>........] - ETA: 0s - loss: 0.3786 - accuracy: 0.9271
899/899 [==============================] - 0s 100us/sample - loss: 0.3664 - accuracy: 0.9344 - val_loss: 0.3796 - val_accuracy: 0.9300
Epoch 10/10

 32/899 [>.............................] - ETA: 0s - loss: 0.3311 - accuracy: 0.9375
736/899 [=======================>......] - ETA: 0s - loss: 0.3333 - accuracy: 0.9524
899/899 [==============================] - 0s 93us/sample - loss: 0.3259 - accuracy: 0.9533 - val_loss: 0.3627 - val_accuracy: 0.9100
Out[1]: <tensorflow.python.keras.callbacks.History at 0x7f87985d47b8>
'''